{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Building a Spell Checker \n",
    "    \n",
    "      \n",
    "This task will involve the creation of a spellchecking system and an evaluation of its performance. The corpus, `holbrook.txt` is an useful one for this NLP task at hand.\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "\n",
    "*<font color=\"blue\">Note:</font> We first preprocess the text and use functions provided by NLTK package to build optimum performing spell checker system. Then evaluate it using few standard techniques*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break down this task in multiple smaller tasks as follows:\n",
    "\n",
    "## Task 1: Import and prepare text data\n",
    "\n",
    "We start by building a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. We consider an example (shown below), where there is only an error in the 10th word and so the list of indexes is [9].\n",
    "\n",
    "<b><font color=\"blue\">Prepare for Evaluation:</font></b>  \n",
    "We split the data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T11:58:24.338024Z",
     "start_time": "2018-03-16T11:58:24.175311Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "#Read from file holbrook.txt\n",
    "with open('holbrook.txt', 'r') as holbrook_txt:\n",
    "    holbrook = holbrook_txt.read().strip()\n",
    "holbrook_lines = holbrook.split('\\n')\n",
    "\n",
    "def find_mistakes(sent):\n",
    "    '''\n",
    "    Function that searches for spelling mistakes (| characters)\n",
    "    input: sent - a line from the text\n",
    "    output: a dcitionary of format (original line, corrected line, index of misspelt words)\n",
    "    '''\n",
    "    dict_corrected = {}\n",
    "    #Token sentence into words\n",
    "    #wordTokenizer = RegexpTokenizer(r'\\w+')\n",
    "    line_words = sent.split(' ')\n",
    "    dict_corrected['original'] = line_words\n",
    "    dict_corrected['corrected'] = line_words\n",
    "    dict_corrected['indexes'] = []\n",
    "    #list comprehension that summarises misspellings eg. [(1, ('siter','sister')), (2, ('go','goes'))]\n",
    "    discrepancies = [(i, w.split('|')) for i,w in enumerate(line_words) if '|' in w]\n",
    "    if len(discrepancies) > 0:\n",
    "        original = line_words.copy()\n",
    "        corrected = line_words.copy()\n",
    "        #Loop through all misspellings in a single sentence\n",
    "        for word_index, word in discrepancies:\n",
    "            original[word_index] = word[0] #misspelt original word eg. siter\n",
    "            corrected[word_index] = word[1] #corrected word eg. sister\n",
    "            dict_corrected['indexes'].append(word_index)\n",
    "        dict_corrected['original'] = original\n",
    "        dict_corrected['corrected'] = corrected\n",
    "    return(dict_corrected)\n",
    "\n",
    "#execute the above defined function for all lines in the holbrook text\n",
    "data = list(map(find_mistakes, holbrook_lines))\n",
    "#Split train/ test/ correct/ incorrect sentences\n",
    "test_correct = [sentence['corrected'] for sentence in data[:100]]\n",
    "train_correct = [sentence['corrected'] for sentence in data[100:]]\n",
    "test_incorrect = [sentence['original'] for sentence in data[:100]]\n",
    "train_incorrect = [sentence['original'] for sentence in data[100:]]\n",
    "\n",
    "#Test this unit of code\n",
    "assert(data[2] == {\n",
    "   'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
    "   'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
    "   'indexes': [9]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T11:58:29.203002Z",
     "start_time": "2018-03-16T11:58:29.183227Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = data[:100]\n",
    "train = data[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2**: Word Frequency Calculation \n",
    "Next, we calculate the frequency (number of occurrences), *ignoring case*, of all words and bigrams (sequences of two words) from the corrected *training* sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T11:58:33.292445Z",
     "start_time": "2018-03-16T11:58:33.159232Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "def unigram(word):\n",
    "    '''Function that returns unigram frequency of the word in the corrected words training corpus'''\n",
    "    fdist_unigrams = FreqDist([word.lower() for line in train_correct for word in line])\n",
    "    return fdist_unigrams[word.lower()]\n",
    "    \n",
    "\n",
    "def bigram(words):\n",
    "    '''Function that returns bigram frequency of the word in the corrected words training corpus'''\n",
    "    fdist_bigrams = FreqDist(ngrams([word.lower() for line in train_correct for word in line],2))\n",
    "    return fdist_bigrams[tuple(nltk.word_tokenize(words))]\n",
    "\n",
    "# Test your code with the following\n",
    "assert(unigram(\"me\")==87)\n",
    "assert(bigram(\"my mother\")==17)\n",
    "assert(bigram(\"you did\")==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 3**: Compute  edit distance\n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T11:58:37.128885Z",
     "start_time": "2018-03-16T11:58:37.094607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above edit distance, we write a function that calculates all words with *minimal* edit distance to the misspelled word. The sub tasks involved are:\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T11:58:41.385091Z",
     "start_time": "2018-03-16T11:58:41.252083Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "def get_candidates(token):\n",
    "    '''\n",
    "    function that calculates all words with minimal edit distance to the misspelled word\n",
    "    input: token - misspelt word\n",
    "    output: list of tokens with minimum edit distance\n",
    "    '''\n",
    "    \n",
    "    #A set of unique tokens in the training data\n",
    "    unique_tokens = set(tok.lower() for line in train_correct for tok in line)\n",
    "    #Calculate edit distances between query token and all other tokens\n",
    "    token_distances = [(edit_distance(token, w), w) for w in unique_tokens]\n",
    "    #Find the minimum edit distance from the misspelt word\n",
    "    min_distance = min(token_distances)[0]\n",
    "    #return all unique tokens with minimum edit distances\n",
    "    return [w for (_, w) in token_distances if _ == min_distance]\n",
    "        \n",
    "# Test your code as follows\n",
    "assert(get_candidates(\"minde\") == ['mind', 'mine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Rudimentary Spell Checker (Baseline version)\n",
    "\n",
    "We build a rudimentary spell checker that takes a (misspelled) sentence and returns the corrected version of that sentence. The system scans the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *bigram probability*. That is the candidate should be selected using the previous and following word in a bigram language model. Thus, if the $i$th word in a sentence is misspelled we use the following to rank candidates:\n",
    "\n",
    "$$p(w_{i+1}|w_i) p(w_i|w_{i-1})$$\n",
    "\n",
    "For the first and last word of the sentence we use only the conditional probabilities that exist.\n",
    "<font color=\"blue\">Note:</font> We just define a baseline version of spell checker which we will enhance with additional natural language processing features and then compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T11:58:45.557955Z",
     "start_time": "2018-03-16T11:58:45.478861Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n",
    "import nltk\n",
    "import string\n",
    "#Find all correct words from training samples\n",
    "all_correct_words = [word for line in train_correct for word in line]\n",
    "#Generate a set of all unique and correct words\n",
    "all_unique_correct_words = [w for w in set(all_correct_words)]\n",
    "#Create bigrams from the corpus words\n",
    "holbrook_bigrams = nltk.bigrams(all_correct_words)\n",
    "#Generate conditional frequency distribution of all bigrams over holbrook text\n",
    "fd_bigrams = nltk.FreqDist(holbrook_bigrams)\n",
    "#Generate conditional probability distribution from frequency distribution\n",
    "cpd_bigrams = ConditionalProbDist(fd_bigrams, MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T11:58:50.118028Z",
     "start_time": "2018-03-16T11:58:49.962071Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def correct(incorrect_sentence):\n",
    "    '''\n",
    "    function that attempts to rectify misspelt words in the sentence\n",
    "    input: a list of words in the incorrect sentence eg. [\"this\",\"whitr\",\"cat\"]\n",
    "    output: a list of words (rectified) in the sentence eg. [\"this\",\"white\",\"cat\"]\n",
    "    '''\n",
    "    #Create a copy of list so that original is not modified\n",
    "    sentence = incorrect_sentence.copy()\n",
    "    \n",
    "    #Word is classified as misspelt if it is not found in training corpus\n",
    "    misspelt_words = [m for m in sentence if m not in all_unique_correct_words]\n",
    "    \n",
    "    #Iterate through each misspelt word and try to correct it\n",
    "    for mw in misspelt_words:\n",
    "        #Find words with minimum edit_distance to misspelt words\n",
    "        #Calculate edit distances between misspelt word and all other words in words corpus\n",
    "        nearest_words = get_candidates(mw)\n",
    "        #Calculate bigram probabilities with all combinations from nearest dictionary words\n",
    "        if sentence.index(mw) == 0:\n",
    "            #if first word in sentence is misspelt, consider combination with only next word\n",
    "            if len(sentence) > 1:\n",
    "                probs = [(cpd_bigrams[w].prob(sentence[1]), w) for w in nearest_words]\n",
    "        elif sentence.index(mw) == (len(sentence) - 1):\n",
    "            #if last word in sentence is misspelt, consider combination with only previous word\n",
    "            probs = [(cpd_bigrams[w].prob(sentence[sentence.index(mw) - 1]), w) for w in nearest_words]\n",
    "        else:\n",
    "            #if misspelt word is neither at start nor at end of sentence\n",
    "            before = sentence[sentence.index(mw) - 1] #previous word\n",
    "            after = sentence[sentence.index(mw) + 1] #next word\n",
    "            probs = [(cpd_bigrams[before].prob(w) * cpd_bigrams[w].prob(after), w) \\\n",
    "                     for w in nearest_words]\n",
    "        if len(sentence) > 1:\n",
    "            #Get the most likely correct word from the key-value pair (probability, candidate_word)\n",
    "            most_likely_correction = max(probs)[1]\n",
    "            #Replace the misspelt word with the most likely candidate\n",
    "            sentence[sentence.index(mw)] = most_likely_correction\n",
    "    return sentence\n",
    "\n",
    "#Test this unit\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 5**: Evaluate the baseline spell-checker \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy of the spellchecker is: 82.197 %\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):\n",
    "    '''\n",
    "    Compute baseline accuracy of the spell checker\n",
    "    input: a list of dictionary that has both, tokens of correct as well as incorrect words\n",
    "    output: accuracy, a decimal value\n",
    "    '''\n",
    "    count_total = 0\n",
    "    count_correct = 0\n",
    "    \n",
    "    for line in test:\n",
    "        rectifiedSentence = correct(line['original'])\n",
    "        for index, word in enumerate(line['corrected']):\n",
    "            count_total += 1\n",
    "            #iterate over all words to find how many words from the \n",
    "            #system's output match the corrected sentence\n",
    "            if rectifiedSentence[index] == word:\n",
    "                count_correct += 1\n",
    "    accuracy = round((count_correct * 100/ count_total),3)    \n",
    "    return accuracy\n",
    "\n",
    "print('Baseline accuracy of the spellchecker is: {} %'.format(accuracy(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## **Task 6: Enhancing the spell-checker with rich NLP features**\n",
    "\n",
    "We now propose modification to the baseline algorithm (developed in Task 3 and 4) that would improve its accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed improvements to the algorithm:\n",
    "\n",
    "### 6.1. Handle issues with proper nouns:\n",
    " - <b><font color=\"blue\">Issue:</font></b> It was observed that system attempted to correct all proper nouns since they could not be found in dictionary and were considered as misspelt words. Algorithm failed miserably as it tried to rectify rarely occuring proper nouns (even those already correct) and returned irrelevant results, bringing down the overall accuracy.\n",
    " - <b><font color=\"green\">Solution:</font></b> Misspellings in proper nouns can be difficult to correct by finding similar candidates as they tend to be rarely used in the training corpus. So, unless the proper nouns occur at least once in the training corpus, it would be impossible to correct them. Hence, all words were <b>*pos-tagged*</b> before searching in dictionary, and words excluding NNP and NNPS (proper nouns singular and plural) were seeked in the dictionary. In this way, algorithm was not allowed to compute suitable replacement candidates for these words and names those were already correct, were saved from being misspelt. Example follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "<b><font color=blue>Baseline Algorithm:</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I live at your area near taller .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I live at Boar Parva near Smallerden .\"\n",
    "' '.join(correct(sentence.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"green\">Improved Algorithm: </font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I live at Boar Parva near Smallerden .'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I live at Boar Parva near Smallerden .\"\n",
    "' '.join(correct_improved(sentence.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.  Handle contractions like (I'll, we'll, she'll, etc.)\n",
    "- <b><font color=\"blue\">Issue:</font></b> Some of the anomalies include word contractions with \"will\" and \"shall\", which have been misspelt without the necessary apostrophe (') punctuation mark. System fails to identify such words (*Ill* for \"I will\" or *well* for \"we shall\") and tries to correct them without any success.     \n",
    "- <b><font color=\"green\">Solution:</font></b> In English language, such contractions (specially with *double ll*) follow a grammar rule.  <br/> <b>Rule: </b>Generally, \"<font color=\"red\">ll</font>\" contraction is attached to a <font color=\"blue\">pronoun</font> and is followed by a <font color=\"green\">verb</font>. eg. <font color=\"blue\">We</font><font color=\"red\">'ll</font> <font color=\"green\">talk</font>, <font color=\"blue\">I</font><font color=\"red\">'ll</font> <font color=\"green\">watch</font> TV, <font color=\"blue\">she</font><font color=\"red\">'ll</font> <font color=\"green\">pay</font> the bill, etc.)<br/>\n",
    "So, I tokenize the sentence and check the sentence for above rule in order to differentiate between actual contractions and words that end in double \"ll\" like \"well\", \"ball\", etc. If the sentence is seen to observe the rule then, such occurences are replaced with apostrophe \"ll\" ('ll), which are recognized as valid spellings by the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "<b><font color=blue>Baseline Algorithm:</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MARY Oh ill have ice-cream and what will you have'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"MARY Oh Ill have ice-cream and what will you have\"\n",
    "' '.join(correct(sentence.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"green\">Improved Algorithm: </font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MARY Oh I'll have ice-cream and what will you have\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"MARY Oh Ill have ice-cream and what will you have\"\n",
    "' '.join(correct_improved(sentence.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T11:59:10.688893Z",
     "start_time": "2018-03-16T11:59:10.591079Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def handle_contractions(flawed_sentence):\n",
    "    '''Function to detect valid contractions of double ll and replace them with apostrophe ll'''\n",
    "    sentence = flawed_sentence.copy()\n",
    "    #find words in sentence that end in \"ll\"\n",
    "    mods = [(w, sentence.index(w)) for w in sentence if w.endswith('ll')]\n",
    "\n",
    "    #Check grammar rule for each instance of above found word\n",
    "    for mod, index_md in mods:\n",
    "        #Extract word part preceding double ll\n",
    "        removed_ll = mod[:-2]\n",
    "        new_sentence = sentence.copy()\n",
    "        #create a new sentence without ll. eg. [I'll, eat] becomes [I, eat]\n",
    "        new_sentence[index_md] = removed_ll\n",
    "        #pos tag the modified sentence\n",
    "        pos_sent = nltk.pos_tag(new_sentence)\n",
    "        if index_md > 1:\n",
    "            previous_tag = pos_sent[index_md][1]\n",
    "            if index_md != (len(new_sentence) - 1):\n",
    "                next_tag = pos_sent[index_md + 1][1]\n",
    "                #Check the grammar rule that states ll is preceded by pronoun and followed by a verb\n",
    "                if next_tag.startswith('VB') and previous_tag.startswith('PRP'):\n",
    "                    sentence[index_md] = \"'\".join([removed_ll,\"ll\"]) #add apostrophe if rule is obeyed\n",
    "        else:\n",
    "            next_tag = pos_sent[index_md + 1][1]\n",
    "            if next_tag.startswith('VB'):\n",
    "                sentence[index_md] = \"'\".join([removed_ll,\"ll\"])\n",
    "    return sentence\n",
    "\n",
    "assert(handle_contractions(word_tokenize(\"MARY Oh Ill have ice-cream and what will you have\")) == ['MARY','Oh',\"I'll\",\n",
    " 'have','ice-cream','and', 'what','will', 'you', 'have'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Use external corpus as dictionary\n",
    "\n",
    "- <b><font color=\"blue\">Issue:</font></b> Some of the valid English words in unseen (heldout) test data that do not occur in training set (that we use as dictionary) are flagged as misspelt words. Then an attempt is made to correct that \"misspelt\" word which when fails, affects the accuracy of the spell checker   \n",
    "- <b><font color=\"green\">Solution:</font></b> Use of large corpus such as \"words\" can help detect if word is truly a valid English word or not and thus forbidding an attempt to correct it. Dictionary is enriched with English language words from larger corpora like <font color=\"brown\">nltk.corpus.words</font> and <font color=\"brown\">nltk.corpus.cmudict</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "<b><font color=blue>Baseline Algorithm:</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T21:21:42.204148Z",
     "start_time": "2018-03-15T21:21:42.094762Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you will have to feed them on a little'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"you will have to feed them on a bottle\"\n",
    "' '.join(correct(sentence.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"green\">Improved Algorithm: </font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T21:22:30.468427Z",
     "start_time": "2018-03-15T21:22:20.610454Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you will have to feed them on a bottle'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"you will have to feed them on a bottle\"\n",
    "' '.join(correct_improved(sentence.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Train with wider range of collocations using external corpus\n",
    "\n",
    "- <b><font color=\"blue\">Issue:</font></b> Some words that commonly co-occur may be missing from the training set, but are encountered in the test set. At such times, the system fails to utilize this sense of co-occurence and returns less probable candidates.\n",
    "- <b><font color=\"green\">Solution:</font></b> It would be better if the model is initially exposed to wider range of collocations in English language. Thus, model is trained with bigrams from popular and relatively larger <font color=\"brown\">\"brown\"</font> corpus in addition to bigrams from Holbrook training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Good-Turing and Kneser–Ney smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T21:43:28.610998Z",
     "start_time": "2018-03-16T21:43:28.590977Z"
    }
   },
   "source": [
    "- <b><font color=\"blue\">Issue:</font></b> Owing to *data sparsity* in training set, probabilities associated with some of the unigrams or bigrams turn up as 0. As calculation of conditional probability involves multiplication, the presense of even a single zero prior results in no likelihood of the candidate word. \n",
    "- <b><font color=\"green\">Solution:</font></b> Smoothing is required to counter the problem of data sparsity and subsequent zero probabilities. Couple of smoothing techinques have been used for improving the spell-correction algorithm: \n",
    "\n",
    "<strong>1. Good Turing Estimation:</strong> It is used to reallocate the probability mass of n-grams that occur r + 1 times in the training data to the n-grams that occur r times. i.e. Some of the probability mass of n-grams that occur at least once is shifted to those that never occured. Thus, none of the n-grams are assigned 0 probabilities. NLTK supports good turing estimation with <font color=\"purple\">\"SimpleGoodTuringProbDist\"</font> module.      \n",
    "\n",
    "- NLTK  method signature:  \n",
    "*<font color=\"purple\">\n",
    "nltk.SimpleGoodTuringProbDist(freqdist, bins=None)   \n",
    ")   *  </font>    \n",
    "<font color=\"red\">:param freqdist:</font> The frequency counts upon which to base the\n",
    "    estimation.       \n",
    "<font color=\"red\">:param bins:</font> The number of possible event types.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "#Find unigram probability distribution with Simple Good Turing estimation\n",
    "#Use all correct words from Holbrook as well sa brown corpus\n",
    "fd_unigrams = nltk.FreqDist(all_correct_words + nltk.corpus.brown.words())\n",
    "pd_unigrams = nltk.SimpleGoodTuringProbDist(fd_unigrams)\n",
    "\n",
    "#Find bigram probability distribution with Simple Good Turing estimation\n",
    "fd_bigrams = nltk.FreqDist(nltk.bigrams(all_correct_words))\n",
    "#add bigram frequencies from holbrook and brown corpuses\n",
    "fd_bigrams = fd_bigrams + nltk.FreqDist(ngrams(nltk.corpus.brown.words(),2))\n",
    "pd_bigrams_smoothed = nltk.SimpleGoodTuringProbDist(fd_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>2. Kneser–Ney smoothing: </strong> This model uses the concept of absolute-discounting interpolation which incorporates information from higher and lower order language models. The addition of the term for lower order n-grams adds more weight to the overall probability when the count for the higher order n-grams is zero. Similarly, the weight of the lower order model decreases when the count of the n-gram is non zero.   \n",
    "   \n",
    " - NLTK method signature:   \n",
    "*<font color=\"purple\">\n",
    "nltk.KneserNeyProbDist(freqdist, bins=None, discount=0.75)  \n",
    ")   *  </font>    \n",
    "<font color=\"red\">:param freqdist:</font> The trigram frequency distribution upon which to base\n",
    "    the estimation   \n",
    "<font color=\"red\">:param bins:</font> The number of possible event types.   \n",
    "<font color=\"red\">:param discount:</font> The discount applied when retrieving counts of\n",
    "    trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find trigram probability distribution with Kneser-Ney smoothing\n",
    "fd_brown_trigrams = nltk.FreqDist(ngrams(nltk.corpus.brown.words(),3))\n",
    "holbrook_trigrams = ngrams(all_correct_words,3)\n",
    "fd_holbrook_trigrams = nltk.FreqDist(holbrook_trigrams)\n",
    "#Combine trigrams from holbrook as well as brown corpus\n",
    "cfd_trigrams = fd_brown_trigrams + fd_holbrook_trigrams\n",
    "cpd_trigrams = nltk.KneserNeyProbDist(cfd_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. Linear interpolation of trigrams, bigrams and unigrams\n",
    "\n",
    "- <b><font color=\"blue\">Issue:</font></b> Certain trigrams are more popular than others. for eg. In sentence, 'I like watching TV', the trigram (to, watch, TV) has more probability of occurence than a trigram in an illogical but syntactically correct sentence 'I like washing TV'. However, this notion is not captured currently by only using bigram probabilities individually.\n",
    "\n",
    "- <b><font color=\"green\">Solution:</font></b>Trigram, bigram and unigram probabilities can be interpolated probability as:\n",
    "\\begin{equation*}\n",
    "P(w|u,v) = \\lambda_1 * P_M(w|u,v) + \\lambda_2 * P_M (w|v) + \\lambda_3 * P_M(w)\n",
    "\\end{equation*}   \n",
    "Thus importance is given to probability distribution of the trigram, bigram as well as unigrams that forms the prior probabilities. So, with respect to example above, \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "0.7 * P(TV | like, washing) + 0.2 * P(TV|washing) + 0.1 * P(TV) < 0.7 * P(TV | like, watching) + 0.2 * P(TV|watching) + 0.1 * P(TV)   \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T20:01:43.041128Z",
     "start_time": "2018-03-16T20:01:40.540933Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a dictionary of English words by taking words occurring in both 'words' as well as CMU dictionary corpus\n",
    "wordDictionary = set(list(set([w[0] for w in nltk.corpus.cmudict.entries()]) & \n",
    "                             set([w.lower() for w in nltk.corpus.words.words()])))\n",
    "dictionaryWords = [w for w in wordDictionary]\n",
    "#Add words from brown corpus to dictionary\n",
    "dictionaryWords += [w.lower() for w in set(nltk.corpus.brown.words())]\n",
    "\n",
    "holbrookCorrectWords = [w for w in all_correct_words if w not in string.punctuation]\n",
    "#Enrich the dictionary with words from Holbrook corpus and keep only unique ones\n",
    "dictionaryAndHolbrookwords = set(dictionaryWords + holbrookCorrectWords)\n",
    "def correct_improved(incorrect_sentence):\n",
    "    '''Improved algorithm for spelling correction\n",
    "    input: a list of word tokens from a sentence\n",
    "    output: a list of 'corrected' word tokens of a sentence\n",
    "    '''\n",
    "    #create a local copy of original sentence to avoid changing it\n",
    "    sentence = incorrect_sentence.copy()\n",
    "    #Handle 'll contractions before further processing\n",
    "    sentence = handle_contractions(sentence)\n",
    "    \n",
    "    #Find misspelt words. ie word that do not occur in enriched dictionary or are not digits/ punctuations\n",
    "    misspelt_words = [m[0] for m in nltk.pos_tag(sentence) if m[1] not in ('NNP', 'NNPS') \\\n",
    "    and m[0].lower() not in dictionaryAndHolbrookwords and \\\n",
    "    m[0] not in string.punctuation and not m[0].isdigit()]\n",
    "    #create a sentence object after stripping original punctuation marks\n",
    "    temp_sentence = [w for w in sentence if w not in string.punctuation]\n",
    "    \n",
    "    #Iterate and try to rectify errors in each misspelt word\n",
    "    for mw in misspelt_words:\n",
    "        #Find edit distance from all correct words in holbrook training set\n",
    "        token_distances = [(edit_distance(mw, w), w) for w in holbrookCorrectWords]\n",
    "        #Find the minimum edit distance from the misspelt word\n",
    "        min_distance = min(token_distances)[0]\n",
    "        #Get all unique words that are at minimum distance from misspelt word\n",
    "        nearest_words = set(w for (_, w) in token_distances if _ == min_distance)\n",
    "        #Calculate the probabilities with all combinations from nearest dictionary words\n",
    "        if temp_sentence.index(mw) == 0:\n",
    "            #if the only word in sentence is misspelt, consider only it for probability\n",
    "            if len(sentence) == 1:\n",
    "                probs = [(pd_unigrams.prob(w), w) for w in nearest_words]\n",
    "            if len(sentence) == 2:\n",
    "            #if the sentence contains only 2 words and first word is misspelt\n",
    "                probs = [(0.9 * pd_bigrams_smoothed.prob((w, sentence[0]))\n",
    "                             + 0.1 * pd_unigrams.prob(w), w) for w in nearest_words]\n",
    "            if len(sentence) > 2:\n",
    "            #if the sentence contains more than 2 words and first word is misspelt\n",
    "            #Calculate interpolated probabilities\n",
    "                probs = [(0.7 * cpd_trigrams.prob((w, sentence[0], sentence[1]))\n",
    "                          + 0.2 * pd_bigrams_smoothed.prob((w, sentence[0]))\n",
    "                             + 0.1 * pd_unigrams.prob(w), w) for w in nearest_words]\n",
    "        elif temp_sentence.index(mw) == (len(temp_sentence) - 1):\n",
    "            #if last word in sentence is misspelt, consider combination with only previous 2 words\n",
    "            probs = [(0.7 * cpd_trigrams.prob((sentence[-3], sentence[-2], w))\n",
    "                      + 0.2 * pd_bigrams_smoothed.prob((sentence[-2], w))\n",
    "                             + 0.1 * pd_unigrams.prob(w), w) for w in nearest_words]\n",
    "        else:\n",
    "        #if misspelt word is neither at start nor at end of sentence            \n",
    "            msindex = temp_sentence.index(mw)\n",
    "        \n",
    "            if msindex == (len(temp_sentence) - 2):\n",
    "            #if penultimate word in sentence is misspelt\n",
    "                         #P(w+1|w) + P(w)\n",
    "                probs = [((0.9 * pd_bigrams_smoothed.prob((w, temp_sentence[msindex + 1]))\n",
    "                             + 0.1 * pd_unigrams.prob(w)) * \\\n",
    "                          #P(w-1|w,w+1) + [P(w|w-1)*P(w+1|w)] + P(w)\n",
    "                          (0.8 * cpd_trigrams.prob((temp_sentence[msindex - 1], w, temp_sentence[msindex + 1]))\n",
    "                          + 0.2 * (pd_bigrams_smoothed.prob((temp_sentence[msindex - 1], w)) \n",
    "                                   * pd_bigrams_smoothed.prob((w, temp_sentence[msindex + 1])))\n",
    "                             + 0.1 * pd_unigrams.prob(w)) * \\\n",
    "                          #P(w|w-2,w-1) + P(w|w-1) + P(w) \n",
    "                          (0.7 * cpd_trigrams.prob((temp_sentence[msindex - 2], temp_sentence[msindex - 1], w))\n",
    "                          + 0.2 * pd_bigrams_smoothed.prob((temp_sentence[msindex - 1], w))\n",
    "                             + 0.1 * pd_unigrams.prob(w)), w) \\\n",
    "                         for w in nearest_words]\n",
    "            else:\n",
    "                #P(w+2|w,w+1) + [P(w+1|w)] + P(w) Misspelt word at the beginning of the trigram\n",
    "                probs = [((0.7 * cpd_trigrams.prob((w, temp_sentence[msindex + 1], temp_sentence[msindex + 2]))\n",
    "                          + 0.2 * pd_bigrams_smoothed.prob((w, temp_sentence[msindex + 1]))\n",
    "                             + 0.1 * pd_unigrams.prob(w)) * \\\n",
    "                #misspelt word in middle of trigram\n",
    "                (0.7 * cpd_trigrams.prob((temp_sentence[msindex - 1], w, temp_sentence[msindex + 1]))\n",
    "                + 0.2 * (pd_bigrams_smoothed.prob((temp_sentence[msindex - 1], w)) \n",
    "                                   * pd_bigrams_smoothed.prob((w, temp_sentence[msindex + 1])))\n",
    "                             + 0.1 * pd_unigrams.prob(w)) * \\\n",
    "                #misspelt word at the end of the trigram\n",
    "                (0.7 * cpd_trigrams.prob((temp_sentence[msindex - 2], temp_sentence[msindex - 1], w))\n",
    "                + 0.2 * pd_bigrams_smoothed.prob((temp_sentence[msindex - 1], w))\n",
    "                             + 0.1 * pd_unigrams.prob(w)), w) \\\n",
    "                for w in nearest_words]\n",
    "        if len(sentence) > 1:\n",
    "            #The correct word is taken to be the one with highest probability mass\n",
    "            most_likely_correction = max(probs)[1]\n",
    "            #Replace the word in original sentence\n",
    "            sentence[sentence.index(mw)] = most_likely_correction\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 7: Evaluate the enchanced spell checker**\n",
    "\n",
    "We repeat the evaluation (as in Task 5) of our new spell checking algorithm and show that it **outperforms** the baseline algorithm from Task 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T21:54:24.920666Z",
     "start_time": "2018-03-16T21:53:30.490017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the improved algorithm is: 91.231%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_improved(test):\n",
    "    '''Evaluate accuracy of the improved algorithm'''\n",
    "    count_total = 0\n",
    "    count_correct = 0\n",
    "    lineNo = 0\n",
    "    listCorrection = []\n",
    "    for line in test:\n",
    "        dictRectification = {}\n",
    "        lineNo += 1\n",
    "        rectifiedSentence = correct_improved(line['original'])\n",
    "        dictRectification['ActuallyCorrect'] = line['corrected']\n",
    "        dictRectification['Original'] = line['original']\n",
    "        dictRectification['SystemRectified'] = rectifiedSentence\n",
    "        #Compare each word in already correct sentence to that of system rectified sentence\n",
    "        for index, word in enumerate(line['corrected']):\n",
    "            count_total += 1\n",
    "            if rectifiedSentence[index] == word:\n",
    "                count_correct += 1\n",
    "        listCorrection.append(dictRectification)\n",
    "    accuracy = round((count_correct * 100/ count_total),3)    \n",
    "    return accuracy\n",
    "\n",
    "print(f'Accuracy of the improved algorithm is: {accuracy_improved(test)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:    \n",
    "The improved algorithm is around 91.23 % accurate in spelling correction task, which is about <font color=\"green\">11% better</font> than original algorithm [82.197%] (in task 3 & 4). Morever, it uses smoothing and interpolation techniques that have lower *perplexity* than the original algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:    \n",
    "\n",
    "1. Prof.Dr. John McCrae & Prof.Dr. Paul Buitelaar, Class notes in module CT5101 Natural Language Processing, National University of Ireland, Galway.\n",
    "2. Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Comput. Speech Lang. 13, 4 (October 1999), 359-394. DOI=http://dx.doi.org/10.1006/csla.1999.0128\n",
    "3. A. Gale, William & Sampson, Geoffrey. (1995). Good-Turing Frequency Estimation Without Tears. Journal of Quantitative Linguistics. 2. 217-237. 10.1080/09296179508590051. \n",
    "4. [NLTK Documentation](https://www.nltk.org/)\n",
    "5. [https://stackoverflow.com/](https://stackoverflow.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
